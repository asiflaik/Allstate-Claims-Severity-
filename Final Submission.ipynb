{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from category_encoders import TargetEncoder,OrdinalEncoder,MEstimateEncoder,JamesSteinEncoder,HashingEncoder,HelmertEncoder\n",
    "from category_encoders import CatBoostEncoder,BaseNEncoder,BackwardDifferenceEncoder\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "- My methodology for this final submission wasfrom Kaggle discussion forum https://www.kaggle.com/brahma0545/encoding-feature-comb-modkzs-1108-72665\n",
    "- I took the combination of randomly selected 35 features as it was computationally efficient\n",
    "- In my earlier submission I have tried building my models without outliers, but after reading most of the kaggle forum discussion I realized removing outliers is causing loss of information and leading to a worst model.\n",
    "- Infact, removing outliers didn't help me much si, I decided to keep them and build a model on top of the m by taking the log of loss value and adding a shift value of 200 which gave a good stability and avoided it from skewness\n",
    "- As my data was highly skewed I munged the skewness by setting some threshold of 0.25\n",
    "- According to the Kaggle Forum most of them have used a different objective function which worked fairly well for optimizing our loss function.\n",
    "- The objective which was used is as below\n",
    "- I will brief more about the objective function used below\n",
    "- In a nutshell, it is y = c * abs(x) - c**2 * np.log(abs(x)/c + 1).\n",
    "- Basically, it means that MAE is not continuously differentiable so we use some approximation.In Xgboost we build some complex objective function for better optimization results.By using the below function it uses the second order derivative for further approximation.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:29:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import itertools\n",
    "\n",
    "#xg_boost = joblib.load('xgboost_with_7folds')\n",
    "xg_boost = joblib.load('boosting_ensemble')\n",
    "\n",
    "shift = 200\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')\n",
    "\n",
    "def encode(charcode):\n",
    "    '''This function simply computes the ordinal value with some modifications'''\n",
    "    r = 0\n",
    "    ln = len(str(charcode))\n",
    "    for i in range(ln):\n",
    "        r += (ord(str(charcode)[i]) - ord('A') + 1) * 26 ** (ln - i - 1)\n",
    "    return r\n",
    "\n",
    "def func_code(X):\n",
    "    '''This function will take any raw input with 130 features and will return standardised data.\n",
    "    Let our data have categorical features or numerical features'''\n",
    "    for comb in itertools.combinations(COMB_FEATURE,2):\n",
    "        feat = comb[0] + '_' + comb[1]\n",
    "        X[feat] = X[comb[0]] + X[comb[1]]\n",
    "        X[feat] = X[feat].apply(encode)\n",
    "    categorical_feats = [x for x in X.columns[0:] if 'cat' in x]\n",
    "    for col in categorical_feats:\n",
    "        X[col] = X[col].apply(encode)\n",
    "        \n",
    "    numeric_feats = [x for x in X.columns[1:-1] if 'cont' in x]\n",
    "    ss = StandardScaler()\n",
    "    X[numeric_feats] = \\\n",
    "        ss.fit_transform(X[numeric_feats].values)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def predict(X,y):\n",
    "    '''Given preprocessed data this function will return predicted values with MAE'''\n",
    "    X = func_code(X)\n",
    "    y = np.log(y)\n",
    "    X = xgb.DMatrix(X)\n",
    "    pred_y = xg_boost.predict(X)\n",
    "    mae = mean_absolute_error(np.exp(y),np.exp(pred_y))\n",
    "    print('The predicted values',np.exp(pred_y))\n",
    "    print('Mean Absolute Error',mae)\n",
    "    return mae,pred_y\n",
    "\n",
    "def predict_values(X):\n",
    "    '''Given preprocessed data this function will return only predicted values'''\n",
    "    X = func_code(X)\n",
    "    X = xgb.DMatrix(X)\n",
    "    pred_y = xg_boost.predict(X)\n",
    "    pred_y = np.exp(pred_y)\n",
    "    return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('train.csv')\n",
    "y = train['loss']\n",
    "train_ = train.drop(['id','loss'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted values [2698.3157 2966.721  4381.2065 1345.6742 3704.241  5027.8755 1471.4421]\n",
      "Mean Absolute Error 763.5435679408478\n",
      "(763.5435679408478, array([7.900383 , 7.9952126, 8.385079 , 7.2046504, 8.217234 , 8.522753 ,\n",
      "       7.2939982], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "X = train_.iloc[:7,:]\n",
    "y = train['loss']\n",
    "y = y.iloc[:7]\n",
    "print(predict(X,y))\n",
    "#print(predict_values(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1833.4105 1628.6284 1340.6573 1507.614  1135.5469 1446.4614 1522.3138]\n"
     ]
    }
   ],
   "source": [
    "print(predict_values(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "- I have tried multiple experiments with the above data for bringing down my performance metric MAE.\n",
    "- As a part of my experimentation I tried removing the outliers and then applying PCA on all of my features including categorical.\n",
    "- Unfortunately, it didn't seem to work that well.So, I tried with different featurizations on data without outliers.\n",
    "- But it didn't seem to work very well as removal of outliers was loss of information and it was not fitting the model as expected.\n",
    "- So, after reading alot of Kaggle discussion and related research papers I found out couple of interesting things which includes the log transform of the target variable and then using np.exp func() to take the original form in eval metric.\n",
    "- Another thing I came across was combination of features and then applying the ordinality technique with some modification.\n",
    "- After reading most of the Kaggle forum discussion and research paper I came across one custom objective function aka Fair Loss.\n",
    "- This fair loss is an approximation of optimizing the second order derivative as MAE is not continuously differentiable.\n",
    "- This Fair Loss is the whole crux of this competition and my self case study too.\n",
    "- This custom objective function alone helped me in showing the drastic improvement in my MAE score with some 7 folds XGBoostRegressor.\n",
    "- Whereas on the other hand my DL models didn't perform very well.\n",
    "- I also tried stacking and ensemble models to improve the further score with some important features obtained from DT but I didn' have enough luck for that.\n",
    "- So, atlast I settled for the rank among top 20% with 1108.71 on Public LB and 1122.50 on Private LB.\n",
    "- For this final function I have written a function in such a way that given any raw input with 130 features and by raw input I mean it can consist categorical features and numerical features. My function predict will return predicted value with MAE and predict_values will only return predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
